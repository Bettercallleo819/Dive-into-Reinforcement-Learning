{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class CliffWalkingEnv:\n",
    "    \"\"\" 悬崖漫步环境\"\"\"\n",
    "    def __init__(self, ncol=12, nrow=4):\n",
    "        self.ncol = ncol  # 定义网格世界的列\n",
    "        self.nrow = nrow  # 定义网格世界的行\n",
    "        # 转移矩阵P[state][action] = [(p, next_state, reward, done)]包含下一个状态和奖励\n",
    "        self.P = self.createP()\n",
    "\n",
    "    def createP(self):\n",
    "        # 初始化\n",
    "        P = [[[] for j in range(4)] for i in range(self.nrow * self.ncol)]\n",
    "        # 4种动作, change[0]:上,change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n",
    "        # 定义在左上角\n",
    "        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        for i in range(self.nrow):\n",
    "            for j in range(self.ncol):\n",
    "                for a in range(4):\n",
    "                    # 位置在悬崖或者目标状态,因为无法继续交互,任何动作奖励都为0\n",
    "                    if i == self.nrow - 1 and j > 0:\n",
    "                        P[i * self.ncol + j][a] = [(1, i * self.ncol + j, 0,\n",
    "                                                    True)]\n",
    "                        continue\n",
    "                    # 其他位置\n",
    "                    next_x = min(self.ncol - 1, max(0, j + change[a][0]))\n",
    "                    next_y = min(self.nrow - 1, max(0, i + change[a][1]))\n",
    "                    next_state = next_y * self.ncol + next_x\n",
    "                    reward = -1\n",
    "                    done = False\n",
    "                    # 下一个位置在悬崖或者终点\n",
    "                    if next_y == self.nrow - 1 and next_x > 0:\n",
    "                        done = True\n",
    "                        if next_x != self.ncol - 1:  # 下一个位置在悬崖\n",
    "                            reward = -100\n",
    "                    P[i * self.ncol + j][a] = [(1, next_state, reward, done)]\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "\n",
    "    def __init__(self, env, theta, gamma):\n",
    "        self.env = env\n",
    "        self.state_num = self.env.ncol * self.env.nrow\n",
    "        self.V = [0] * self.state_num\n",
    "        self.policy = [[0.25, 0.25, 0.25, 0.25] for i in range(self.state_num)]   # initialized as random policy\n",
    "        self.theta = theta\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def policy_evaluation(self):\n",
    "        count = 0\n",
    "        while 1:\n",
    "            max_diff = 0\n",
    "            new_V = [0] * self.state_num\n",
    "            for s in range(self.state_num):\n",
    "                q_value_list = []\n",
    "                for a in range(4):\n",
    "                    q_value = 0\n",
    "                    for res in self.env.P[s][a]:\n",
    "                        p, next_state, r, done = res \n",
    "                        q_value += p * (r + self.gamma * self.V[next_state] * (1 - done))\n",
    "                    q_value_list.append(self.policy[s][a] * q_value)\n",
    "                new_V[s] = sum(q_value_list)\n",
    "                max_diff = max(max_diff, abs(new_V[s] - self.V[s]))\n",
    "            self.V = new_V\n",
    "            if max_diff < self.theta:break\n",
    "            count += 1\n",
    "        print(\"After\",count,\"terms of iteration, the policy evaluation is done!\")\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        for s in range(self.state_num):\n",
    "            q_value_list = []\n",
    "            for a in range(4):\n",
    "                q_value = 0\n",
    "                for res in self.env.P[s][a]:\n",
    "                    p, next_state, r, done = res \n",
    "                    q_value += p * (r + self.gamma * self.V[next_state]) * (1-done)\n",
    "                q_value_list.append(q_value)\n",
    "            max_q_value = max(q_value_list)\n",
    "            max_count = q_value_list.count(max_q_value)\n",
    "            self.policy[s] = [1/ max_count if q == max_q_value else 0 for q in q_value_list]\n",
    "        print(\"Policy improvement is done!\")\n",
    "        return self.policy\n",
    "\n",
    "    def policy_iteration(self):\n",
    "        while 1:\n",
    "            self.policy_evaluation()\n",
    "            old_policy = copy.deepcopy(self.policy)\n",
    "            new_policy = self.policy_improvement()\n",
    "            if old_policy == new_policy:\n",
    "                break\n",
    "        print(\"Policy iteration is done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_agent(agent, action_meaning, disaster=[], end=[]):\n",
    "    print(\"状态价值：\")\n",
    "    for i in range(agent.env.nrow):\n",
    "        for j in range(agent.env.ncol):\n",
    "            # 为了输出美观,保持输出6个字符\n",
    "            print('%6.6s' % ('%.3f' % agent.V[i * agent.env.ncol + j]), end=' ')\n",
    "        print()\n",
    "\n",
    "    print(\"策略：\")\n",
    "    for i in range(agent.env.nrow):\n",
    "        for j in range(agent.env.ncol):\n",
    "            # 一些特殊的状态,例如悬崖漫步中的悬崖\n",
    "            if (i * agent.env.ncol + j) in disaster:\n",
    "                print('****', end=' ')\n",
    "            elif (i * agent.env.ncol + j) in end:  # 目标状态\n",
    "                print('EEEE', end=' ')\n",
    "            else:\n",
    "                a = agent.policy[i * agent.env.ncol + j]\n",
    "                pi_str = ''\n",
    "                for k in range(len(action_meaning)):\n",
    "                    pi_str += action_meaning[k] if a[k] > 0 else 'o'\n",
    "                print(pi_str, end=' ')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 59 terms of iteration, the policy evaluation is done!\n",
      "Policy improvement is done!\n",
      "After 71 terms of iteration, the policy evaluation is done!\n",
      "Policy improvement is done!\n",
      "After 43 terms of iteration, the policy evaluation is done!\n",
      "Policy improvement is done!\n",
      "After 12 terms of iteration, the policy evaluation is done!\n",
      "Policy improvement is done!\n",
      "After 0 terms of iteration, the policy evaluation is done!\n",
      "Policy improvement is done!\n",
      "Policy iteration is done!\n",
      "状态价值：\n",
      "-7.712 -7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n",
      "-7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n",
      "-7.712 -100.0 -100.0 -100.0 -100.0 -100.0 -100.0 -100.0 -100.0 -100.0 -100.0 -1.000 \n",
      "-100.0  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n",
      "策略：\n",
      "ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \n",
      "ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ovoo \n",
      "^ooo ovoo ovoo ovoo ovoo ovoo ovoo ovoo ovoo ovoo ovoo ovoo \n",
      "ooo> **** **** **** **** **** **** **** **** **** **** EEEE \n"
     ]
    }
   ],
   "source": [
    "env = CliffWalkingEnv()\n",
    "action_meaning = ['^', 'v', '<', '>']\n",
    "theta = 0.001\n",
    "gamma = 0.9\n",
    "agent = PolicyIteration(env, theta, gamma)\n",
    "agent.policy_iteration()\n",
    "print_agent(agent, action_meaning, list(range(37, 47)), [47])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('RLlab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38d6941677519051cdd29ca2e142688905d4d903c4cc48e23749f6348f344da9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
